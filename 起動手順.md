# ダッシュボードをターミナルで起動する手順

---

## コピペ用プロンプト（最短）

**すでに `npm install` 済みで、すぐ起動したいとき** — 以下をターミナルに貼り付けて Enter：

```bash
cd /Users/hidenobumizuki/Downloads/sample_downloads/dashboard-template && npm run dev
```

起動したらブラウザで **http://localhost:3000** を開く。

---

## 初回セットアップ＋起動（1行）

**初めて使う or 依存関係を入れ直したいとき** — 以下をターミナルに貼り付けて Enter：

```bash
cd /Users/hidenobumizuki/Downloads/sample_downloads/dashboard-template && npm install && npm run dev
```

---

## 1. プロジェクトフォルダへ移動

```bash
cd /Users/hidenobumizuki/Downloads/sample_downloads/dashboard-template
```

（別の場所にクローンしている場合は、そのパスに読み替えてください。）

---

## 2. 依存関係のインストール（初回のみ）

```bash
npm install
```

---

## 3. 環境変数の設定（初回または変更時）

```bash
cp .env.example .env
```

`.env` を開き、以下を編集してください。

- **必須（ログイン用）**
  - `NEXTAUTH_SECRET` … 32文字以上のランダム文字列（例: `openssl rand -base64 32` で生成）
  - `NEXTAUTH_URL` … ローカルなら `http://localhost:3000`
  - `AUTH_USERNAME` / `AUTH_PASSWORD` … ログインするユーザー名・パスワード

- **連絡・ファイル解析で AI を使う場合**
  - `OPENAI_API_KEY` … OpenAI の API キー

- **クローズド運用（外部に送りたくない場合）**
  - `CLOSED_AI_MODE=true`

- **Ollama でローカル解析する場合**（既に .env に設定済み）
  - `LOCAL_AI_BASE_URL=http://localhost:11434/v1`
  - `LOCAL_AI_MODEL=llama3.2`（画像解析もする場合は `llava`）
  - 詳細は `docs/OLLAMA_SETUP.md` または `./scripts/setup-ollama.sh` を参照

---

## 4. （Ollama を使う場合）Ollama のインストールと起動

ローカルで解析する場合は、**先に** Ollama を入れてモデルを取得し、サーバーを起動してください。

```bash
# インストール（Homebrew がある場合）
brew install ollama

# サーバー起動（別ターミナルで常時実行）
ollama serve

# モデル取得（初回のみ・数分かかることがあります）
ollama pull llama3.2
```

一括実行: `chmod +x scripts/setup-ollama.sh && ./scripts/setup-ollama.sh`

---

## 5. 開発サーバーを起動

```bash
npm run dev
```

ブラウザで **http://localhost:3000** を開くと、ログイン画面が表示されます。

---

## 6. 本番ビルドして起動する場合

```bash
npm run build
npm start
```

---

## よく使うコマンド一覧

| 目的           | コマンド        |
|----------------|-----------------|
| 開発サーバー起動 | `npm run dev`   |
| 本番ビルド     | `npm run build` |
| 本番起動       | `npm start`     |
| リント         | `npm run lint`  |
